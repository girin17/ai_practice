import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msnodf=pd.read_csv('london_merged.csv',parse_dates=['timestamp'])
df.head()print('데이터 구조는":' ,df.shape)
print('데이터의 타입은:',df.dtypes)
print('데이터의 컬럼은', df.columns)

# 각 컬럼별 결측치 -> null인 부분이 없어서 아마 00000
df.isna().sum()

# msno.matrix는 대규모 데이터 세트에서 누락된 데이터의 패턴을 빠르게 시각적으로 분석하는 데 매우 유용
msno.matrix(df)
plt.show()


# df['이름'] 새로운 컬럼을 생성 -> year, month, dayofweek를 생성
df['year']=df['timestamp'].dt.year
df['month']=df['timestamp'].dt.month
df['dayofweek']=df['timestamp'].dt.dayofweek
df['hour']=df['timestamp'].dt.hour

df.head()
df['weather_code'].value_counts()#  plt.subplots
# 하나이상의 그래프를 포함할수 있는 figure 객체 하나 또는 여러개의 Axes 객체 생성
# 1,1: 하나의 행과 열로 구성된 그래프 격자를 만든다는 의미 
    # -> 결과적으로는 하나의 Axes를 생성
# figsize: 그래프 크기
a, b = plt.subplots(1, 1, figsize=(10, 5)) 

# seaborn의 boxplot 함수
#  대부분의 인자들이 키워드로 명시되어야 함
#  sns.boxplot(df['year'], df['cnt']) => 이렇게 쓰면 오류남

# boxplot: 데이터 분포와 이상치를 시각화 하는데 사용 
#b라는 Axes위에 박스플롯을 그리라는 의미
sns.boxplot(x=df['year'], y=df['cnt'], ax=b)  #year: 년도 , cnt: 자전거 이용객
# sns.barplot(x=df['year'], y=df['cnt'], ax=b)  #year: 년도 , cnt: 자전거 이용객
# sns.barplot(x='year',y='cnt', data=df)

plt.show()
# case1 
# df['year']: DataFrame에서 직접적으로 열을 선택 
# 열의 이름을 문자열로 전달하는 것이 아니라 열자체를 직접 참조
# sns.boxplot(x=df['year'], y=df['cnt'], ax=b)  

# case2

# sns.barplot(x=feature,y='cnt', data=data,palette='Set3',orient='v')

# 그래프 함수 만들기
def plot_bar(data,feature):
    fig=plt.figure(figsize=(12,3))
    # df(csv)데이터를 이용해 hour데이터의 barplot 작성
    sns.barplot(x=feature,y='cnt', data=data,palette='Set3',orient='v')
   # case1 
# df['year']: DataFrame에서 직접적으로 열을 선택 
# 열의 이름을 문자열로 전달하는 것이 아니라 열자체를 직접 참조
# sns.boxplot(x=df['year'], y=df['cnt'], ax=b)  

# case2

# sns.barplot(x=feature,y='cnt', data=data,palette='Set3',orient='v')

# 그래프 함수 만들기
def plot_bar(data,feature):
    fig=plt.figure(figsize=(12,3))
    # df(csv)데이터를 이용해 hour데이터의 barplot 작성
    sns.barplot(x=feature,y='cnt', data=data,palette='Set3',orient='v')
   # 아웃라이어 제거
def is_outlier(s):
    lower_limit=s.mean()-(s.std()*3) # 평균의 3배 뺀값 (이상치)
    upper_limit=s.mean()+(s.std()*3)# 평균의 3배 더한값 (이상치)
    return ~s.between(lower_limit,upper_limit)# 시간대별 이용객수를 그룹화 해서
# is_outlier(이상치 제거함수)에 적용해서 df_out에 대입
df_out=df[~df.groupby('hour')['cnt'].transform(is_outlier)]  # 인강에선 .transform 대신 apply 썻엇음(난 오류남)

print("이상치 제거전:" , df.shape)
print("이상치 제거후:" ,df_out.shape)

# 결과
# 이상치 제거전: (17414, 14) # 각각 행렬의 개수를 의미
# 이상치 제거후: (17265, 14)# 시간대별 이용객수를 그룹화 해서
# is_outlier(이상치 제거함수)에 적용해서 df_out에 대입
df_out=df[~df.groupby('hour')['cnt'].transform(is_outlier)]  # 인강에선 .transform 대신 apply 썻엇음(난 오류남)

print("이상치 제거전:" , df.shape)
print("이상치 제거후:" ,df_out.shape)

# 결과
# 이상치 제거전: (17414, 14) # 각각 행렬의 개수를 의미
# 이상치 제거후: (17265, 14)# df_out는 이상치 제거된 data로 이제부터 이것을 이용할것

df_out['weather_code']=df_out['weather_code'].astype('category')
df_out['season']=df_out['season'].astype('category')
df_out['year']=df_out['year'].astype('category')
df_out['month']=df_out['month'].astype('category')
df_out['hour']=df_out['hour'].astype('category')



print(df_out.dtypes)
df_out['hour']# get_dummies: 범주형을 수치형 데이터로 변환하기 위해서

df_out=pd.get_dummies(df_out,columns=['weather_code','season','year','month','hour']) 
 # 다시 돌리면 오류날수밖에 없음 이미 실행된 코드라 weather_code등의 칼럼이 나뉘었기 때문df_out.head()
# 종속변수 df_y : 모델이 예측하려는 종속변수(여기선 cnt라는 칼럼)
df_y= df_out['cnt']

# 독립변수 df_x:  cnt(종속변수)와  timestamp 칼럼을 제거한 모든 열            /////-> timestamp(우리가 시간을 찍으려고 만들었던 칼럼임)
df_x= df_out.drop(['timestamp','cnt'],axis=1)  # axis=1 열방향으로 데이터 제거한다는 의미


df_x.head() # cnt,timestamp 제거된 모든 칼럼 출력
df_y.head() # cnt


# 훈련용 , 테스트용 데이터 분리

from sklearn.model_selection import train_test_split

# df_x: cnt,timestamp 제거된 모든 칼럼(독립변수)print(df_out.shape)
# 70대 30 비율로 x y가 나뉨
print('x_train의 구조는',x_train.shape)  # 70% , 독립변수 학습
print('y_train의 구조는',y_train.shape) # 70% , 종속변수 학습
print('x_test의 구조는',x_test.shape) # 30% , 독립변수 학습
print('y_test 구조는',y_test.shape) # 30% , 종속변수 학습

# df_y: cnt (종속변수)

x_train, x_test,y_train, y_test=train_test_split(df_x,df_y,random_state=66,test_size=0.3,shuffle=False)
#  옵션설명
 # random_state: 같은 시드값으로 섞으면 매번동일한 결과를 얻음
 # test_size: 훈련, 테스트 비율설정 0.3:0.7로 각각 변수에 대입됌
 # shuffle: 데이터는 원본 순서대로 분할된다. : 시계열이므로 안섞이게 false로 설정
    # | shuffle-true인 경우: 데이터가 무작위로 섞여 각 분할에 데이터가 고르게 분포되도록 함(훈련과정에 편향을 줄이고 일반화 성능을 높이는데 도움이 됨)import tensorflow as tf
from tensorflow import keras

from keras.models import Sequential # 딥러닝 층을쌓아서 하는 방식으로 진행할것임
from keras.layers import Dense
from keras.callbacks import EarlyStopping  # 중간에 과적합이 되지 않도록 사용하는 라이브러리

model= Sequential()
# 층을 쌓음
 
model.add(Dense(units=160,activation='relu',input_dim=57)) # 1층
model.add(Dense(units=60,activation='relu'))# 2층
model.add(Dense(units=20,activation='relu'))# 3층

# 마지막층 - 시간대별 자전거 수요를 예측하는것 이므로 1개로 결과가 나와야함 -> units=1
model.add(Dense(units=1,activation='linear'))
model.compile(loss='mae',optimizer='adam',metrics=['mae'])
early_stopping=EarlyStopping(monitor='loss',patience=5,mode='min')


# x_train: 70% , 독립변수 학습데이터
# y_train: 70% , 종속변수 학습데이터
# 신경망을 훈련시키는 fit 함수
#history=model.fit(x_train,y_train,epochs=50, batch_size=1,validation_split=0.1,callbacks=[early_stopping]) 
history=model.fit(x_train,y_train,epochs=5, batch_size=1,validation_split=0.1,callbacks=[early_stopping]) 


# 옵션 설명
# 1. epochs: 모델이 학습 데이터 셋을 전체적으로 50번 반복해 학습 (한번의 에폭: 전체 데이터 셋을 한번 통과하는 것을 의미) 
# 2. batch_size=1 : 학습데이터 셋에서 한번에 1개의 샘플씩 가중치를 업데이트하는데 사용된다.

# 3.validation_split=0.1:
## 70%의 훈련데이터 x_train에서도 10%를 검증 데이터로 분리하여 사용
## 모델이 학습하는 동안 성능을 검증하는 데 사용되며, 모델이 훈련데이터에 과적합되는걸 방지

# 4. callbacks=[early_stopping]: 콜백은 학습을 조기에 종료시키기 위해 사용
plt.plot(history.history['val_loss'])
plt.plot(history.history['loss'])
plt.title('loss 비교')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend(['val_loss','loss'])
plt.show()

from sklearn.metrics import mean_squared_error

def RMSE(y_test,y_predict):
    return np.sqrt(mean_squared_error(y_test,y_predict))
print("RMSE:",RMSE(y_test,y_predict))
from sklearn.ensemble import RandomForestRegressor
rf= RandomForestRegressor(n_estimators=100,random_state=16)
rf.fit(x_train,y_train)
rf_result=rf.predict(x_test)
print('RMSE',RMSE(y_test,rf_result))from xgboost import XGBRegressor
xgb= XGBRegressor(n_eetimators=100, random_state=16)
xgb.fit(x_train,y_train)
xgb_result= xgb.predict(x_test)
print('RMSE',RMSE(y_test,xgb_result))

from lightgbm import LGBMRegressor
lgb = LGBMRegressor(n_estimators=100, random_state=16)
lgb.fit(x_train,y_train)
lgb_result=lgb.predict(x_test)
print('RMSE',RMSE(y_test,lgb_result))
# 메이비 각 모델별 모형을 비교하는게 아닐까?


xgb=pd.DataFrame(xgb_result)
rf=pd.DataFrame(rf_result)
dnn=pd.DataFrame(y_predict)
compare=pd.DataFrame(y_test).reset_index(drop=True)compare['xgb'] =xgb
compare['rf'] =rf
compare['dnn']=dnn

compare.head()sns.kdeplot(compare['cnt'],shade=True,color='r')
sns.kdeplot(compare['xgb'],shade=True,color='b')
sns.kdeplot(compare['rf'],shade=True,color='y')
sns.kdeplot(compare['dnn'],shade=True,color='g')
sns.kdeplot(compare['lgb'],shade=True,color='ro')

