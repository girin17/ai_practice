import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msnodf=pd.read_csv('london_merged.csv',parse_dates=['timestamp'])
df.head()df.shape #(17414, 10)
# df.shape 의 결과: (17414, 10)

# a.ipynb에서는 train:test를 7:3으로 나눴었는데
# 여기에는 17000개를 train ,  414개를 test로 
# 1:2 는 cnt 열을 의미
train =df.iloc[:17000,1:2]
test =df.iloc[17000:17414,1:2]

print(train.shape)
print(test.shape)# 17414개에 대한 자전거 이용 cnt 데이터
df['cnt'][:17000].plot(figsize=(15,4),legend=True) # 15*4인치 그래프로 그림
df['cnt'][17000:].plot(figsize=(15,4),legend=True)
# legend: 범례로 각각의 수치가 무엇을 나타내는지 알려줌
plt.legend(['train','test'])
plt.title('bike share demand') 
plt.show()
from statsmodels.tsa.stattools import pacf

# 자전거 이용 고객 데이터(CNT는 고객수의 해당하는 칼럼) 17000여개가 
# 시간에 따라 나열되있으므로 시계열 데이터가 맞음


# 1. df['cnt']: 분석하려는 시계열 데이터
# 2. nlags: 시차의 개수 | PACF를 계산할때 고려할 최대 시차수를 20으로 설정
   ## 자전거 데이터의 경우 시간 단위이므로 20시간 시차를 고려하겠다는 것으로 해석(?)됌
   ## 시계열이 아닌 경우:  nlags=20은 단순히 '이전 20개 데이터 포인트'까지의 관계를 고려하겠다는 의미가 됌

# 3. ols(Ordinary Least Squares):일반 최소 제곱법
   ## 회귀 분석에서 가장 기본적이고 널리 사용되는 방법
   ## 하나이상의 독립변수와 종속변수 사이의 관계를 모델링 할때, 
   ## 관측된 데이터와 모델 예측 사이의 제곱 오차의 합을 최소화하는 파라미터를 찾아냄


pacf = pacf(df['cnt'], nlags=20, method='ols')
print(pacf)
from statsmodels.graphics.tsaplots import plot_pacf

# cnt열의 부분자기상관함수를 계산하고 그래프로 그리는 지시 - 시각화
plot_pacf(df['cnt'], lags=20, method='ols', title='Partial Autocorrelation Function').show()



# plot_pacf(pacf, lags=20, method='ols', title='Partial Autocorrelation Function').show()from sklearn.preprocessing import MinMaxScaler

sc=MinMaxScaler(feature_range=(0,1)) # 데이터를 0과 1 사이의 값으로 스케일링하겠다는 의미
train_scaled =sc.fit_transform(train) # train: cnt의 17000번 데이터까지 담긴 변수


# fit():17000개 데이터중 train에 대해 최소값과 최대값을 계산합니다.
# transform(): 계산된 최소값과 최대값을 사용하여 데이터를 0과 1 사이로 변환합니다. 

train_scaled # 배열값인 array로 결과값 나옴
train_scaled.shapefrom sklearn.preprocessing import MinMaxScaler

sc=MinMaxScaler(feature_range=(0,1)) # 데이터를 0과 1 사이의 값으로 스케일링하겠다는 의미
train_scaled =sc.fit_transform(train) # train: cnt의 17000번 데이터까지 담긴 변수


# fit():17000개 데이터중 train에 대해 최소값과 최대값을 계산합니다.
# transform(): 계산된 최소값과 최대값을 사용하여 데이터를 0과 1 사이로 변환합니다. 
x_train=[]
y_train=[]

# train_scaled.shape 의 결과가 (17000, 1)로 train_scaled[n:0]뒤에 0은 사실상 특별한 의미는 없음
for i in range(1,17000):
    # train 데이터를 MinMaxScaler 적용시킨 train_scaled

    x_train.append(train_scaled[i-1:i,0]) # 데이터 1개전 shift된값을 x_train(독립변수)   #  여기선 1시간 단위의 데이터가 기록되어 있으므로 1시간 전 데이터라고 해석 가능
    y_train.append(train_scaled[i,0]) # 일반적 데이터 y에 (종속변수)

# x_train, y_train: 리스트 -> np array 형태로 바꾸는거임
x_train,y_train=np.array(x_train),np.array(y_train)


## 시계열은 3차원의 데이터를 필요로함
x_train.shape #(16999, 1)로 2차원 데이터임

# 3차원 데이터로 변경
x_train=np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))

x_train.shape #(16999, 1, 1)

# 전처리 완료(정리하자면)
##### 17000개 cnt 데이터를 MinMaxScaler로 0과 1 사이 데이터로 바꿔서
##### 시계열 데이터 처리를 위한 3차원 형태 .reshape으로 바꿈# Keras API가 TensorFlow 내부에 통합되어 인강과 작성법 변경
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Activation
rnn=Sequential() # 레이어를 선형으로 쌓는 간단한 방법을 제공
# 입->출력으로 데이터가 순차적으로 흐르는 모델을 쉽게 만들수 있게 해줌


# SimpleRNN: 기본적인 rnn레이어로 순차적인 연결 포함
# relu라는 활성함수를 사용
# units=6 -> 뉴런의 개수를 6개로 (출력차원이 6이된다?)
  ## layer과 units의 관계
     ### 1. layer: 입력 데이터를 받아 처리하고 출력을 다음 레이어로 전달하는 데이터 처리 모듈
     ### 2. units: 뉴런과 같은 의미로 레이어 안에 있는 뉴런, 해당 레이어가 수행할수 있는 계산의 기본단위

# input_shape: 여기선 RNN 모델에 데이터가 어떻게 들어오는지를 설명
 ##  input(타임스텝수, 특성수) 
   ## 1. 타임스텝스: 각 입력 시퀀스에 포함될 연속적인 데이터 포인트의 수(대부분의 경우, 특히 시계열 데이터를 한 번에 한 단계씩 처리할 때는 이 값을 1로 설정)
   ## 2. 특성수: 영향을 미치는 특성 > 예를들어 여기선 시간에 대한 자전거 이용수 이므로 특성이 1이지만 | 온도,습도,시간같은 특성이 많아지면 3같이 둘수 있다
rnn.add(SimpleRNN(activation='relu',units=6,input_shape=(1,1)))



# Dense레이어 
# 1. activation: 입력받은 값을 그대로 출력함을 의미: 회귀문제에서 주로 사용 (마지막 출력일때 이 활성화 함수 사용)
# 2. units=1:  레이어가 가질 출력유닛(뉴런)의 수를 정의 -> 보통 최종 예측값 하나를 출력하기 위해 사용
rnn.add(Dense(activation='linear',units=1)) 

print(rnn.summary())
# 순환 신경망 모델 학습 전에 모델을 설정하는 단계
# mse: Mean Squared Error'의 약자로, 예측값과 실제값의 차이를 제곱하여 평균낸 것
#   =>값이 작을수록 모델의 성능이 좋다고 평가됩니다.

# metrics=['mse']: 학습 및 테스트 단계에서 모델을 평가하기 위한 지표를 지정
# 이 지표를 통해 모델의 성능을 모니터링할 수 있음

# 'adam' 최적화기는 이 손실을 효과적으로 최소화하기 위해 모델의 가중치를 조정


#rnn으로 학습시키기
rnn.compile(loss='mse',optimizer='adam',metrics=['mse'])
# x_train: 자전거 이용객의 1시간 전으로 shift된 데이터 (독립변수)
# y_train : 원래데이터
rnn.fit(x_train,y_train,batch_size=1,epochs=2)inputs=sc.transform(testtest)
inputs.shape  #(414, 1) 행렬 개수
# 테스트용 데이터 x_test도  1시간 전으로 shift(독립변수) 시킨후 3차원 배열로 형태 변경
x_test=[]
for i in range(1,415):
    x_test.append(inputs[i-1:i,0])
x_test=np.array(x_test)
x_test=np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))
x_test.shape# 3차원으로 바꿔준 데이터를 rnn에다 예측을 해서 대입을 해주고
rnn=rnn.predict(x_test)
# 0에서 1로 >MinMaxScaler< 스케일을 작업햇는데 이를 다시 원래값으로 돌려서 rnn으로 넣음
rnn=sc.inverse_transform(rnn)# test는 df.iloc[17000:17414,1:2]

test=np.array(test) 

plt.figure(figsize=(15,5))
plt.plot(test,marker='.',label='cnt',color='black') # 실제데이터 그래프
plt.plot(rnn,marker='.',label='RNN',color='red') # 예측 그래프
plt.legend()
